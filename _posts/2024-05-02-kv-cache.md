---
layout: default
title:  "KV cache"
description: "An engineering trick in decoding: KV cache"
categories: decoding
tags: kv-cache
excerpt_separator: <!--more-->
---

TL;DR;

Cache/Avoid the repeating operations of keys and values during decoding in auto-regreesive models.

---

Summary

1. To generate $n_{k}$ token, we need to calculate the keys and values of $\{ n_{1} ... n_{k} \}$ and the query of $n_{k}$.
2. The keys and values of $\{n_{1} ... n_{k-1} \}$ are actually calculated when generating $n_{k-1}$, so it can be cahced.
3. Because we have cached key and values in the previous steps, we don't need to save previous tokens
4. KV caches takes `$2 * b * h * l * n * s * 2$` bytes for float16

---

Memory usuage

For example, if b(batch_size)=1, h(num_of_heads)=96, l(num_of_layers)=96, n(num_of_dimension)=12888, s(sequence length)=512 and float16, kv-cache takes ~226 GB. 

