---
layout: default
title:  "NeRF"
description: "Neural Radiance Fields"
categories: nerf paper
tags: nerf 2020
excerpt_separator: <!--more-->
---

TL; DR;

NERF is a method to generate realistic 3D scenes from 2D images.

---

Summary

1. `Raidiance Fields`: NeRF represents a scene as a continuous volumetric field, where each point in space has a certain density and color (raidance)
2. `The Neural Network`: The neural network takes a 3D coordinate $(x, y, z)$ and a viewing function $(\theta, \phi)$ as input and outputs the color and the volumn density at that point.
3. `Volumn Rendering`: Given an  image, it simulate that every pixel is composed of every point along with a ray towards a direction $d$
4. `Training with 3D images`: NeRF is trained using multiple 2D images of a scene taken from different viewpoints.

--- 

Questions

1. `Training Time`: Takes around 100-300k iteration, 1-2 days on a high-end GPU (e.g. NVIDIA V100)
2. `Number of 2D images needed`: At least 100
3. `Model archi.`: 8 layers MLP to first generate the desenity output and a 256-dimensional feature vector. This vector is tehn concatenated with the camera ray's viewing direction and passed to 1 additional fully-connected layer that output the view-dependent RGB color.
4. `Model size`: Only requires around 5 MB for network weights

---

Reference

The papaer is published in 2020 [NeRF: Representing Scenes as
Neural Radiance Fields for View Synthesis](https://arxiv.org/pdf/2003.08934)